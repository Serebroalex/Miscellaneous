{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34e1d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading -cartoon -drawing -vector capybara images: 100%|█| 250/250 [00:50<00\n",
      "Downloading -capybara animal images: 100%|███| 25/25 [00:06<00:00,  3.97image/s]\n",
      "Downloading -capybara objects images: 100%|██| 25/25 [00:06<00:00,  3.70image/s]\n",
      "Downloading -capybara city images: 100%|█████| 50/50 [00:11<00:00,  4.35image/s]\n",
      "Downloading -capybara nature images: 100%|███| 50/50 [00:11<00:00,  4.19image/s]\n",
      "Downloading -capybara human images: 100%|████| 50/50 [00:10<00:00,  4.80image/s]\n",
      "Downloading -capybara food images: 100%|█████| 50/50 [00:10<00:00,  4.66image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H5 dataset saved as capybara_dataset.h5\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Capybara image dataset generator.\n",
    "This notebook downloads images of capybaras and not capybaras and produces an h5 dataset of labelled images.\n",
    "\"\"\"\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def calculate_image_hash(image_bytes):\n",
    "    \"\"\"\n",
    "    Calculate a unique hash for an image.\n",
    "    \"\"\"\n",
    "    hasher = hashlib.md5()\n",
    "    hasher.update(image_bytes)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def download_images(query, num_images = 1, save_path = './images'):\n",
    "    \"\"\"\n",
    "    Downoad num_images of unique images requested in query. Save to save_path.\n",
    "    \"\"\"\n",
    "    # Create a folder for the query if it doesn't exist\n",
    "    if save_path == './images':\n",
    "        query_folder = os.path.join(save_path, query)\n",
    "    else:\n",
    "        query_folder = os.path.join(save_path)\n",
    "        \n",
    "    os.makedirs(query_folder, exist_ok=True)\n",
    "    \n",
    "    # Counter for downloaded images\n",
    "    count = 0\n",
    "\n",
    "    # Set to store downloaded image hashes\n",
    "    downloaded_image_hashes = set()\n",
    "\n",
    "    # Initialize the page counter\n",
    "    page = 0\n",
    "\n",
    "    with tqdm(total = num_images, desc = f\"Downloading {query} images\", unit = \"image\") as pbar:\n",
    "        while count < num_images:\n",
    "            url = f\"https://www.google.com/search?q={query}&tbm=isch&start={page * 20}\" \n",
    "\n",
    "            try:\n",
    "                # Send a GET request with a User-Agent header\n",
    "                headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "                response = requests.get(url, headers = headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                img_tags = soup.find_all(\"img\")\n",
    "\n",
    "                if img_tags:\n",
    "                    for img_tag in img_tags:\n",
    "                        if count >= num_images:\n",
    "                            break\n",
    "\n",
    "                        img_link = img_tag.get(\"src\")\n",
    "                        \n",
    "                        if img_link and img_link.startswith(\"http\"):\n",
    "                            response = requests.get(img_link)\n",
    "                            response.raise_for_status()\n",
    "                            image_bytes = response.content\n",
    "\n",
    "                            # Calculate the hash of the image\n",
    "                            image_hash = calculate_image_hash(image_bytes)\n",
    "\n",
    "                            # Check if the image hash has already been downloaded\n",
    "                            if image_hash not in downloaded_image_hashes:\n",
    "                                count += 1\n",
    "                                downloaded_image_hashes.add(image_hash)\n",
    "\n",
    "                                img = Image.open(BytesIO(image_bytes))\n",
    "                                img.save(os.path.join(query_folder, f'{query}_{count}.png'))\n",
    "                                logging.info(f'{query} - Downloaded image {count}')\n",
    "                                pbar.update(1)\n",
    "                            else:\n",
    "                                logging.info(f'{query} - Skipped duplicate image')\n",
    "                else:\n",
    "                    logging.warning(f\"Couldn't find any images for: {query}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(f\"Error fetching images for {query}: {e}\")\n",
    "\n",
    "            # Increment the page counter for the next iteration\n",
    "            page += 1\n",
    "\n",
    "            # Add a 2 second sleep time between requests\n",
    "            time.sleep(2)\n",
    "\n",
    "def load_and_preprocess_images(folder):\n",
    "    \"\"\"\n",
    "    Load and preprocess images in a specified folder.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".jpeg\") or filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img = cv2.imread(os.path.join(folder, filename))\n",
    "            img = cv2.resize(img, (64, 64))  # Resizing to (64, 64)\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure logging\n",
    "    logging.basicConfig(filename = 'image_downloader.log', level = logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    query = 'capybara'\n",
    "    limit = 250\n",
    "\n",
    "    download_images('-cartoon -drawing -vector ' + query, num_images = limit, save_path='./images/capybara')\n",
    "    download_images(f'-{query} animal', num_images = int(.1 * limit), save_path='./images/not_capybara')\n",
    "    download_images(f'-{query} objects', num_images = int(.1 * limit), save_path='./images/not_capybara')\n",
    "    download_images(f'-{query} city', num_images = int(.2 * limit), save_path='./images/not_capybara')\n",
    "    download_images(f'-{query} nature', num_images = int(.2 * limit), save_path='./images/not_capybara')\n",
    "    download_images(f'-{query} human', num_images = int(.2 * limit), save_path='./images/not_capybara')\n",
    "    download_images(f'-{query} food', num_images = int(.2 * limit), save_path='./images/not_capybara')\n",
    "\n",
    "    # Define the paths to your image folders\n",
    "    capybara_folder = \"./images/capybara\"\n",
    "    non_capybara_folder = \"./images/not_capybara\"\n",
    "\n",
    "    # Define the H5 dataset file names\n",
    "    h5_dataset_filename = \"capybara_dataset.h5\"\n",
    "\n",
    "    # Load capybara and non-capybara images\n",
    "    capybara_images = load_and_preprocess_images(capybara_folder)\n",
    "    non_capybara_images = load_and_preprocess_images(non_capybara_folder)\n",
    "\n",
    "    classes = [\"not capybara\", \"capybara\"]\n",
    "\n",
    "    # Create an H5 file for the train dataset and store the images and labels\n",
    "    with h5py.File(h5_dataset_filename, \"w\") as h5file:\n",
    "        h5file.create_dataset(\"x\", data = np.array(capybara_images + non_capybara_images))\n",
    "        h5file.create_dataset(\"y\", data = np.array([1] * len(capybara_images) + [0] * len(non_capybara_images)))\n",
    "        h5file.create_dataset(\"list_classes\", data = classes, dtype = h5py.special_dtype(vlen=str))\n",
    "\n",
    "    print(f\"H5 dataset saved as {h5_dataset_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
